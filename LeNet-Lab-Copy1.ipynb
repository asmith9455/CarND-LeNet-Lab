{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Lab\n",
    "![LeNet Architecture](lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the MNIST data, which comes pre-loaded with TensorFlow.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Image Shape: (28, 28, 1)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABZxJREFUeJztnE1oVFcUx39/bQNqg7TW1NDEtsRuQhAbQwS7ySZQutBWMNZFLVJiN2K6EBqCgWyEtrRuC5YKXQRiJYXqxlKkQREMWomNqaaVUBJjsFSFTIpQ05wu5k0+zMdM5s3cmfdyf/B4M/fNe/fMf86cd8+dM1dmhscNqwptwErCi+0QL7ZDvNgO8WI7xIvtEC+2Q0KJLektSYOS7khqzZVRcUXZJjWSVgO/A43AXeAqsN/MfsudefHimRDn1gN3zGwIQFIXsBtYVGxJsU1XzUzpXhMmjLwMjMx6fjdom4OkQ5KuSboWoq9YEMazF/ok53mumZ0ETkK8PTsTwnj2XaBy1vMK4F44c+JNGLGvAq9Lek1SCfAecDY3ZsWTrMOImU1KOgz8CKwGTpnZQM4siyFZD/2y6izGMTvfoxHPMvFiO8SL7RAvtkO82A7xYjvEi+0QL7ZDwkxEOeXSpUsA7Ny5c96xCxcuADAwMJPAXrx4EYAbN24AkEgkAHj48CFr1qwBYGJiIn8GL4D3bIdEJl0fHx8HYO3atZn2BUDq/Q0PDwMwODjIhg0bALh3LzlJeezYMQBu3ryZrXk+XS82IhOzOzs7AdixYwcABw4cmD6W8tTW1uRvzo2NjfPO37x585w9QG1tLQD9/f1AOM/OBO/ZDomMZ58+fRqAK1euAAt74eXLlwEoKSmhpaVlwescPXqU9evX58nKpYnMDTIsZWVlAJw/f56tW7cC8ODBAwB27doFQG9vb9bX9zfIIiMyYSRbNm3aBEBHRwfAtFcDNDc3A+E8ejl4z3ZIbGP2qlVJP+rq6gJgz54908dSntzQ0ADAkydPQveXScyOrdh1dXXAzOglRSKRYPv27QAMDQ3lrD9/gywyYnuD7O7uXrD9+PHjOfXo5eA92yGx9OyDBw9SUVEBzMz6tbW1AXDixImC2ZXWsyVVSvpZ0i1JA5JagvYXJP0k6Y9g/3z+zY04ZrbkBpQDtcHjUpL/NqgGPgdag/ZW4LMMrmX53GpqaqympsYePXpkKdrb2629vT2v/SZlXPq9m1n6MGJmY8BY8Dgh6RbJovfdQEPwsm+BHuCTdNfLJ0eOHAGgtLSUqakpAPr6+gpp0hyWFbMlvQq8AfQCLwUfBGY2JqlskXMOAYfCmRkPMhZb0nNAN/CxmY2nfnZKh4t/HqTmPfbt2zfdNjo6CkBPT08+usyKjIZ+kp4lKXSnmX0fNN+XVB4cLwf+yo+J8SGtZyvpwt8At8xs9rjpLPAB8Gmw/yEvFqZBElVVVQCsW7duur2pqQmYKWEoBjIJI28C7wP9klJ3mzaSIn8n6UNgGNibHxPjQ+QnorZs2cLt27fntI2NjVFZWbnIGflhRUxE7d07/wt15syZAliSnsiLHSkyyXxytZGHzG1oaMgmJyfnbNXV1XnPGJ/eMnn/3rMdEtlZv/r6egA2btw43ZaqWB0ZGVnwnEITWbFTJWap8l+YyRaLaWw9Gx9GHBJZz06Vmj1+/Jhz584BM6W/xYr3bIdEPoMsFlZEBhklvNgO8WI7xIvtENdDv7+Bf4J9sfMimdv5SiYvcjoaAZB0zczqnHaaBfmw04cRh3ixHVIIsU8WoM9syLmdzmP2SsaHEYc4E7uY19peolK3Q9KopL5geztUPy7CSLGvtR1UdJWb2XVJpcAvwDtAEzBhZl/koh9Xnj291raZ/Quk1touCsxszMyuB48TQKpSN6e4EjujtbaLgacqdQEOS/pV0qmwBf+uxM5ore1C83SlLvAVUAVsI1mj/mWY67sSu+jX2l6oUtfM7pvZf2Y2BXxNMhxmjSuxi3qt7cUqdVMl0QHvAqFWf3Ey6xeBtbYXq9TdL2kbyZD3J/BRmE58BukQn0E6xIvtEC+2Q7zYDvFiO8SL7RAvtkO82A75HzSoB5xd9FQ7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):\n",
    "    \n",
    "    print('starting LeNet forward pass')\n",
    "    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    \n",
    "    w1 = tf.Variable(tf.random_normal([5, 5, 1, 6]))\n",
    "    b1 = tf.Variable(tf.zeros(6))\n",
    "\n",
    "    conv_1 = tf.nn.conv2d(x, w1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    \n",
    "    print('conv_1 shape A: ', conv_1.get_shape()[1:4])\n",
    "    assert(conv_1.get_shape()[1:4] == [28, 28, 6])\n",
    "    \n",
    "    conv_1 = tf.nn.bias_add(conv_1, b1)\n",
    "    \n",
    "    print('conv_1 shape B: ', conv_1.get_shape()[1:4])\n",
    "    assert(conv_1.get_shape()[1:4] == [28, 28, 6])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    \n",
    "    conv_1 = tf.nn.relu(conv_1)\n",
    "    \n",
    "    print('conv_1 shape C: ', conv_1.get_shape()[1:4])\n",
    "    assert(conv_1.get_shape()[1:4] == [28, 28, 6])\n",
    "\n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    \n",
    "    pool_1 = tf.nn.max_pool(conv_1,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')\n",
    "\n",
    "    print('pool_1 shape: ', pool_1.get_shape()[1:4])\n",
    "    assert(pool_1.get_shape()[1:4] == [14, 14, 6])\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Input = 14x14x6. Output = 10x10x16.\n",
    "    \n",
    "    w2 = tf.Variable(tf.random_normal([5, 5, 6, 16]))\n",
    "    b2 = tf.Variable(tf.zeros(16))\n",
    "    \n",
    "    conv_2 = tf.nn.conv2d(pool_1, w2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    \n",
    "    conv_2 = tf.nn.bias_add(conv_2, b2)\n",
    "    \n",
    "    print('conv_2 shape A: ', conv_2.get_shape()[1:4])\n",
    "    assert(conv_2.get_shape()[1:4] == [10, 10, 16])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    \n",
    "    conv_2 = tf.nn.relu(conv_2)\n",
    "    \n",
    "    print('conv_2 shape B: ', conv_2.get_shape()[1:4])\n",
    "    assert(conv_2.get_shape()[1:4] == [10, 10, 16])\n",
    "\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    \n",
    "    pool_2 = tf.nn.max_pool(conv_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    print('pool_2 shape: ', pool_2.get_shape()[1:4])\n",
    "    assert(pool_2.get_shape()[1:4] == [5, 5, 16])\n",
    "\n",
    "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
    "    \n",
    "    #flat = tf.reshape(pool_2, [-1, 1, 400]) #should be a row vector with 400 elements\n",
    "    flat = flatten(pool_2)\n",
    "    \n",
    "    print('flat shape: ', flat.get_shape()[1:3])\n",
    "    #assert(flat.get_shape()[1:3] == [1, 400])\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    \n",
    "    w_fc1 = tf.Variable(tf.random_normal([400,120]))\n",
    "    b_fc1 = tf.Variable(tf.zeros(120))\n",
    "    \n",
    "    fc1 = tf.matmul(flat, w_fc1)\n",
    "    fc1 = tf.add(fc1, b_fc1)\n",
    "    \n",
    "    print('fc1 shape: ', fc1.get_shape())\n",
    "    #assert(fc1.get_shape() == [1, 120])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    \n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    print('fc1 shape: ', fc1.get_shape())\n",
    "    #assert(fc1.get_shape() == [1, 120])\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    \n",
    "    w_fc2 = tf.Variable(tf.random_normal([120,84]))\n",
    "    b_fc2 = tf.Variable(tf.zeros(84))\n",
    "    \n",
    "    fc2 = tf.matmul(fc1, w_fc2)\n",
    "    fc2 = tf.add(fc2, b_fc2)\n",
    "    \n",
    "    print('fc2 shape: ', fc2.get_shape())\n",
    "    #assert(fc2.get_shape() == [1, 84])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    \n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    print('fc2 shape: ', fc2.get_shape())\n",
    "    #assert(fc2.get_shape() == [1, 84])\n",
    "\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    \n",
    "    w_fc3 = tf.Variable(tf.random_normal([84,10]))\n",
    "    b_fc3 = tf.Variable(tf.zeros(10))\n",
    "    \n",
    "    fc3 = tf.matmul(fc2, w_fc3)\n",
    "    logits = tf.add(fc3, b_fc3)\n",
    "    \n",
    "    print('logits shape: ', logits.get_shape())\n",
    "    #assert(logits.get_shape() == [1, 10])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting LeNet forward pass\n",
      "conv_1 shape A:  (28, 28, 6)\n",
      "conv_1 shape B:  (28, 28, 6)\n",
      "conv_1 shape C:  (28, 28, 6)\n",
      "pool_1 shape:  (14, 14, 6)\n",
      "conv_2 shape A:  (10, 10, 16)\n",
      "conv_2 shape B:  (10, 10, 16)\n",
      "pool_2 shape:  (5, 5, 16)\n",
      "flat shape:  (400,)\n",
      "fc1 shape:  (?, 120)\n",
      "fc1 shape:  (?, 120)\n",
      "fc2 shape:  (?, 84)\n",
      "fc2 shape:  (?, 84)\n",
      "logits shape:  (?, 10)\n"
     ]
    }
   ],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /gpu:0\n",
      "Training...\n",
      "Batch Size:  1024\n",
      "starting EPOCH 1 ...\n",
      "end EPOCH 1 ...\n",
      "Validation Accuracy = 0.339\n",
      "\n",
      "starting EPOCH 2 ...\n",
      "end EPOCH 2 ...\n",
      "Validation Accuracy = 0.530\n",
      "\n",
      "starting EPOCH 3 ...\n",
      "end EPOCH 3 ...\n",
      "Validation Accuracy = 0.634\n",
      "\n",
      "starting EPOCH 4 ...\n",
      "end EPOCH 4 ...\n",
      "Validation Accuracy = 0.702\n",
      "\n",
      "starting EPOCH 5 ...\n",
      "end EPOCH 5 ...\n",
      "Validation Accuracy = 0.746\n",
      "\n",
      "starting EPOCH 6 ...\n",
      "end EPOCH 6 ...\n",
      "Validation Accuracy = 0.772\n",
      "\n",
      "starting EPOCH 7 ...\n",
      "end EPOCH 7 ...\n",
      "Validation Accuracy = 0.800\n",
      "\n",
      "starting EPOCH 8 ...\n",
      "end EPOCH 8 ...\n",
      "Validation Accuracy = 0.818\n",
      "\n",
      "starting EPOCH 9 ...\n",
      "end EPOCH 9 ...\n",
      "Validation Accuracy = 0.832\n",
      "\n",
      "starting EPOCH 10 ...\n",
      "end EPOCH 10 ...\n",
      "Validation Accuracy = 0.840\n",
      "\n",
      "starting EPOCH 11 ...\n",
      "end EPOCH 11 ...\n",
      "Validation Accuracy = 0.852\n",
      "\n",
      "starting EPOCH 12 ...\n",
      "end EPOCH 12 ...\n",
      "Validation Accuracy = 0.858\n",
      "\n",
      "starting EPOCH 13 ...\n",
      "end EPOCH 13 ...\n",
      "Validation Accuracy = 0.866\n",
      "\n",
      "starting EPOCH 14 ...\n",
      "end EPOCH 14 ...\n",
      "Validation Accuracy = 0.872\n",
      "\n",
      "starting EPOCH 15 ...\n",
      "end EPOCH 15 ...\n",
      "Validation Accuracy = 0.880\n",
      "\n",
      "starting EPOCH 16 ...\n",
      "end EPOCH 16 ...\n",
      "Validation Accuracy = 0.882\n",
      "\n",
      "starting EPOCH 17 ...\n",
      "end EPOCH 17 ...\n",
      "Validation Accuracy = 0.886\n",
      "\n",
      "starting EPOCH 18 ...\n",
      "end EPOCH 18 ...\n",
      "Validation Accuracy = 0.890\n",
      "\n",
      "starting EPOCH 19 ...\n",
      "end EPOCH 19 ...\n",
      "Validation Accuracy = 0.896\n",
      "\n",
      "starting EPOCH 20 ...\n",
      "end EPOCH 20 ...\n",
      "Validation Accuracy = 0.899\n",
      "\n",
      "starting EPOCH 21 ...\n",
      "end EPOCH 21 ...\n",
      "Validation Accuracy = 0.899\n",
      "\n",
      "starting EPOCH 22 ...\n",
      "end EPOCH 22 ...\n",
      "Validation Accuracy = 0.904\n",
      "\n",
      "starting EPOCH 23 ...\n",
      "end EPOCH 23 ...\n",
      "Validation Accuracy = 0.907\n",
      "\n",
      "starting EPOCH 24 ...\n",
      "end EPOCH 24 ...\n",
      "Validation Accuracy = 0.910\n",
      "\n",
      "starting EPOCH 25 ...\n",
      "end EPOCH 25 ...\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "starting EPOCH 26 ...\n",
      "end EPOCH 26 ...\n",
      "Validation Accuracy = 0.914\n",
      "\n",
      "starting EPOCH 27 ...\n",
      "end EPOCH 27 ...\n",
      "Validation Accuracy = 0.917\n",
      "\n",
      "starting EPOCH 28 ...\n",
      "end EPOCH 28 ...\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "starting EPOCH 29 ...\n",
      "end EPOCH 29 ...\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "starting EPOCH 30 ...\n",
      "end EPOCH 30 ...\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "starting EPOCH 31 ...\n",
      "end EPOCH 31 ...\n",
      "Validation Accuracy = 0.922\n",
      "\n",
      "starting EPOCH 32 ...\n",
      "end EPOCH 32 ...\n",
      "Validation Accuracy = 0.923\n",
      "\n",
      "starting EPOCH 33 ...\n",
      "end EPOCH 33 ...\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "starting EPOCH 34 ...\n",
      "end EPOCH 34 ...\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "starting EPOCH 35 ...\n",
      "end EPOCH 35 ...\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "starting EPOCH 36 ...\n",
      "end EPOCH 36 ...\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "starting EPOCH 37 ...\n",
      "end EPOCH 37 ...\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "starting EPOCH 38 ...\n",
      "end EPOCH 38 ...\n",
      "Validation Accuracy = 0.929\n",
      "\n",
      "starting EPOCH 39 ...\n",
      "end EPOCH 39 ...\n",
      "Validation Accuracy = 0.931\n",
      "\n",
      "starting EPOCH 40 ...\n",
      "end EPOCH 40 ...\n",
      "Validation Accuracy = 0.932\n",
      "\n",
      "starting EPOCH 41 ...\n",
      "end EPOCH 41 ...\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "starting EPOCH 42 ...\n",
      "end EPOCH 42 ...\n",
      "Validation Accuracy = 0.934\n",
      "\n",
      "starting EPOCH 43 ...\n",
      "end EPOCH 43 ...\n",
      "Validation Accuracy = 0.934\n",
      "\n",
      "starting EPOCH 44 ...\n",
      "end EPOCH 44 ...\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "starting EPOCH 45 ...\n",
      "end EPOCH 45 ...\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "starting EPOCH 46 ...\n",
      "end EPOCH 46 ...\n",
      "Validation Accuracy = 0.938\n",
      "\n",
      "starting EPOCH 47 ...\n",
      "end EPOCH 47 ...\n",
      "Validation Accuracy = 0.938\n",
      "\n",
      "starting EPOCH 48 ...\n",
      "end EPOCH 48 ...\n",
      "Validation Accuracy = 0.938\n",
      "\n",
      "starting EPOCH 49 ...\n",
      "end EPOCH 49 ...\n",
      "Validation Accuracy = 0.938\n",
      "\n",
      "starting EPOCH 50 ...\n",
      "end EPOCH 50 ...\n",
      "Validation Accuracy = 0.939\n",
      "\n",
      "starting EPOCH 51 ...\n",
      "end EPOCH 51 ...\n",
      "Validation Accuracy = 0.938\n",
      "\n",
      "starting EPOCH 52 ...\n",
      "end EPOCH 52 ...\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "starting EPOCH 53 ...\n",
      "end EPOCH 53 ...\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "starting EPOCH 54 ...\n",
      "end EPOCH 54 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "starting EPOCH 55 ...\n",
      "end EPOCH 55 ...\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "starting EPOCH 56 ...\n",
      "end EPOCH 56 ...\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "starting EPOCH 57 ...\n",
      "end EPOCH 57 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "starting EPOCH 58 ...\n",
      "end EPOCH 58 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "starting EPOCH 59 ...\n",
      "end EPOCH 59 ...\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "starting EPOCH 60 ...\n",
      "end EPOCH 60 ...\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "starting EPOCH 61 ...\n",
      "end EPOCH 61 ...\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "starting EPOCH 62 ...\n",
      "end EPOCH 62 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "starting EPOCH 63 ...\n",
      "end EPOCH 63 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "starting EPOCH 64 ...\n",
      "end EPOCH 64 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "starting EPOCH 65 ...\n",
      "end EPOCH 65 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "starting EPOCH 66 ...\n",
      "end EPOCH 66 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "starting EPOCH 67 ...\n",
      "end EPOCH 67 ...\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "starting EPOCH 68 ...\n",
      "end EPOCH 68 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 69 ...\n",
      "end EPOCH 69 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 70 ...\n",
      "end EPOCH 70 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 71 ...\n",
      "end EPOCH 71 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "starting EPOCH 72 ...\n",
      "end EPOCH 72 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 73 ...\n",
      "end EPOCH 73 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 74 ...\n",
      "end EPOCH 74 ...\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "starting EPOCH 75 ...\n",
      "end EPOCH 75 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 76 ...\n",
      "end EPOCH 76 ...\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "starting EPOCH 77 ...\n",
      "end EPOCH 77 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 78 ...\n",
      "end EPOCH 78 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "starting EPOCH 79 ...\n",
      "end EPOCH 79 ...\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "starting EPOCH 80 ...\n",
      "end EPOCH 80 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "starting EPOCH 81 ...\n",
      "end EPOCH 81 ...\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "starting EPOCH 82 ...\n",
      "end EPOCH 82 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "starting EPOCH 83 ...\n",
      "end EPOCH 83 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "starting EPOCH 84 ...\n",
      "end EPOCH 84 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 85 ...\n",
      "end EPOCH 85 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 86 ...\n",
      "end EPOCH 86 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 87 ...\n",
      "end EPOCH 87 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "starting EPOCH 88 ...\n",
      "end EPOCH 88 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 89 ...\n",
      "end EPOCH 89 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 90 ...\n",
      "end EPOCH 90 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 91 ...\n",
      "end EPOCH 91 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 92 ...\n",
      "end EPOCH 92 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "starting EPOCH 93 ...\n",
      "end EPOCH 93 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 94 ...\n",
      "end EPOCH 94 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 95 ...\n",
      "end EPOCH 95 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 96 ...\n",
      "end EPOCH 96 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 97 ...\n",
      "end EPOCH 97 ...\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "starting EPOCH 98 ...\n",
      "end EPOCH 98 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 99 ...\n",
      "end EPOCH 99 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "starting EPOCH 100 ...\n",
      "end EPOCH 100 ...\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print(\"Batch Size: \", BATCH_SIZE)\n",
    "    for i in range(EPOCHS):\n",
    "        print(\"starting EPOCH {} ...\".format(i+1))\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"end EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.961\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
